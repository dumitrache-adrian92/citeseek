{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
   "metadata": {
    "id": "0730d6a1-c893-4840-9817-5e5251676d5d"
   },
   "source": [
    "### Installation\n",
    "\n",
    "The LangChain VertexAI integration lives in the `langchain-google-vertexai` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652d6238-1f87-422a-b135-f5abbb8652fc",
   "metadata": {
    "id": "652d6238-1f87-422a-b135-f5abbb8652fc",
    "outputId": "d9b7a3d5-7fc3-4e87-811b-016af3648b2c",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m95.4/95.4 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m7.3/7.3 MB\u001B[0m \u001B[31m30.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cde65-254d-4219-a441-068766c0d4b5",
   "metadata": {
    "id": "a38cde65-254d-4219-a441-068766c0d4b5"
   },
   "source": [
    "## Instantiation\n",
    "\n",
    "Now we can instantiate our model object and generate chat completions:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T16:29:59.347925Z",
     "start_time": "2025-03-26T16:29:59.338073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import vertexai\n",
    "\n",
    "# Set up the VertexAI client\n",
    "vertexai.init(\n",
    "    project=\"disco-direction-454210-k6\",\n",
    "    location=\"europe-central2\",\n",
    ")"
   ],
   "id": "7cd0f3913a2ace13",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "outputId": "a246d56c-3915-4398-8dd4-407fcf5b7c6a",
    "ExecuteTime": {
     "end_time": "2025-03-26T16:30:02.507753Z",
     "start_time": "2025-03-26T16:30:00.682762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0, # not much reason to have randomness in this kind of classifier\n",
    "    max_tokens=256,\n",
    "    max_retries=6,\n",
    "    stop=None,\n",
    ")"
   ],
   "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {
    "id": "2b4f3e15"
   },
   "source": [
    "## Invocation\n",
    "\n",
    "We'll be evaluating the model using multiple prompts."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:02:52.259486Z",
     "start_time": "2025-03-27T15:02:52.251600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts = [\"Is the given sentence from a scientific paper missing a citation marker?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           \"Does the following sentence require a citation marker?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           \"Should I add a citation marker to this sentence?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           \"Does the given sentence reference a different scientific paper?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           ]"
   ],
   "id": "80d1e7f2ef56eb97",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "tags": [],
    "id": "62e0dbc3",
    "outputId": "e76997e2-c318-413b-ae3c-4a4f97ba33d8",
    "ExecuteTime": {
     "end_time": "2025-03-26T16:30:06.597102Z",
     "start_time": "2025-03-26T16:30:06.593762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_prediction(prompt, sentence):\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            sentence,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    ai_msg = llm.invoke(messages)\n",
    "\n",
    "    return ai_msg.content.strip() == \"yes\""
   ],
   "id": "62e0dbc3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
    "outputId": "d865141c-9ef3-4a62-8183-febacea4793e",
    "ExecuteTime": {
     "end_time": "2025-03-26T16:30:09.746706Z",
     "start_time": "2025-03-26T16:30:08.042315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Change to the path where the dataset is stored\n",
    "DATASET_PATH = \"C:\\\\Users\\\\Adrian\\\\Documents\\\\datasets\\\\citing_test.parquet\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_parquet(DATASET_PATH)\n",
    "\n",
    "# get first 500 rows\n",
    "df = df.head(500)\n",
    "\n",
    "df.describe()"
   ],
   "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 sentence citing\n",
       "count                                                 500    500\n",
       "unique                                                500      2\n",
       "top     Under these assumptions, we have the following...  False\n",
       "freq                                                    1    470"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>citing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Under these assumptions, we have the following...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# evaluate each prompt\n",
    "preds = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    predictions = df[\"sentence\"].apply(lambda x: generate_prediction(prompt, x))\n",
    "    preds.append(predictions)"
   ],
   "id": "fb821a2cfafa54c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "Evaluating each prompt, the clear winner is the last one (`Does the given sentence reference a different scientific paper?`). Unfortunately, going strictly by metrics, it's worse than the finetuned scibert."
   ],
   "id": "85d462e7deb7dc56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T13:23:13.213221Z",
     "start_time": "2025-03-26T13:23:13.161005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for prompt, predictions in zip(prompts, preds):\n",
    "    print(prompt)\n",
    "\n",
    "    report = classification_report(df[\"citing\"], predictions)\n",
    "\n",
    "    print(report)\n",
    "    print()"
   ],
   "id": "13a8330b40fae620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the given sentence from a scientific paper missing a citation marker?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.24      0.39       470\n",
      "        True       0.07      0.93      0.14        30\n",
      "\n",
      "    accuracy                           0.28       500\n",
      "   macro avg       0.53      0.59      0.26       500\n",
      "weighted avg       0.93      0.28      0.37       500\n",
      "\n",
      "\n",
      "Does the following sentence require a citation marker?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.57      0.72       470\n",
      "        True       0.09      0.63      0.15        30\n",
      "\n",
      "    accuracy                           0.57       500\n",
      "   macro avg       0.52      0.60      0.43       500\n",
      "weighted avg       0.91      0.57      0.68       500\n",
      "\n",
      "\n",
      "Should I add a citation marker to this sentence?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.39      0.56       470\n",
      "        True       0.08      0.87      0.15        30\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.53      0.63      0.36       500\n",
      "weighted avg       0.93      0.42      0.53       500\n",
      "\n",
      "\n",
      "Does the given sentence reference a different scientific paper?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.94      0.96       470\n",
      "        True       0.38      0.57      0.45        30\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.67      0.75      0.70       500\n",
      "weighted avg       0.94      0.92      0.93       500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
