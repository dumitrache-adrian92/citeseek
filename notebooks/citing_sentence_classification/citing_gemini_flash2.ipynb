{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0730d6a1-c893-4840-9817-5e5251676d5d",
   "metadata": {
    "id": "0730d6a1-c893-4840-9817-5e5251676d5d"
   },
   "source": [
    "# Gemini Flash 2.0 for citing sentence classification\n",
    "\n",
    "This notebook attempts to determine if simply prompting Gemini is capable of producing a good results in detecting citing sentences.\n",
    "\n",
    "We will be evaluating multiple system prompts with low temperature on a dataset of sentences extracted from scientific papers and labeled as citing or non-citing.\n",
    "\n",
    "For simplicity, we'll be using Langchain to call this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38cde65-254d-4219-a441-068766c0d4b5",
   "metadata": {
    "id": "a38cde65-254d-4219-a441-068766c0d4b5"
   },
   "source": [
    "## Instantiation\n",
    "\n",
    "Using Google AI products requires the Google Cloud SDK to be installed on your system.\n",
    "\n",
    "The following code initializes the Vertex project (you can choose any project you want, since we're just prompting a base model that should be available in any project) and chooses a datacenter."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:30:22.631947Z",
     "start_time": "2025-03-30T15:30:20.109820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import vertexai\n",
    "\n",
    "# Set up the VertexAI clie`nt\n",
    "vertexai.init(\n",
    "    project=\"disco-direction-454210-k6\", # any project should work\n",
    "    location=\"europe-central2\", # adjust based on your location\n",
    ")"
   ],
   "id": "7cd0f3913a2ace13",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll be prompting Gemini with a null temperature to avoid any randomness, as we want a straight answer for the most part.",
   "id": "a0f3793c1c540259"
  },
  {
   "metadata": {
    "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "outputId": "a246d56c-3915-4398-8dd4-407fcf5b7c6a",
    "ExecuteTime": {
     "end_time": "2025-03-30T15:30:24.598053Z",
     "start_time": "2025-03-30T15:30:22.856578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0, # not much reason to have randomness in this kind of classifier\n",
    "    max_tokens=256, # we'll see immediately that our system prompts request a yes or no answer, so we don't need a lot of tokens\n",
    "    max_retries=6,\n",
    "    stop=None,\n",
    ")"
   ],
   "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {
    "id": "2b4f3e15"
   },
   "source": [
    "## Invocation\n",
    "\n",
    "We'll be trying a couple of different system prompts, but all of them will be prompted to give a simple yes/no answer with no further thinking."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:30:26.282149Z",
     "start_time": "2025-03-30T15:30:26.278497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompts = [\"Is the given sentence from a scientific paper missing a citation marker?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           \"Does the following sentence require a citation marker?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           \"Should I add a citation marker to this sentence?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           \"Does the given sentence reference a different scientific paper?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\",\n",
    "           ]"
   ],
   "id": "80d1e7f2ef56eb97",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "tags": [],
    "id": "62e0dbc3",
    "outputId": "e76997e2-c318-413b-ae3c-4a4f97ba33d8",
    "ExecuteTime": {
     "end_time": "2025-03-30T15:30:27.468557Z",
     "start_time": "2025-03-30T15:30:27.462928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_prediction(sys_prompt, sentence):\n",
    "    messages = [\n",
    "        (\n",
    "            \"system\",\n",
    "            sys_prompt,\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            sentence,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    ai_msg = llm.invoke(messages)\n",
    "\n",
    "    return ai_msg.content.strip() == \"yes\""
   ],
   "id": "62e0dbc3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predictions",
   "id": "d8cf66c9445c1a90"
  },
  {
   "metadata": {
    "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
    "outputId": "d865141c-9ef3-4a62-8183-febacea4793e",
    "ExecuteTime": {
     "end_time": "2025-03-30T15:30:33.539767Z",
     "start_time": "2025-03-30T15:30:32.106003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Change to the path where the dataset is stored\n",
    "DATASET_PATH = \"C:\\\\Users\\\\Adrian\\\\Documents\\\\datasets\\\\citing_test.parquet\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_parquet(DATASET_PATH)\n",
    "\n",
    "# get first 500 rows\n",
    "df = df.head(500)\n",
    "\n",
    "df.describe()"
   ],
   "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 sentence citing\n",
       "count                                                 500    500\n",
       "unique                                                500      2\n",
       "top     Under these assumptions, we have the following...  False\n",
       "freq                                                    1    470"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>citing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Under these assumptions, we have the following...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:42:49.970335Z",
     "start_time": "2025-03-30T15:34:03.244972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# evaluate each prompt\n",
    "sys_prompt_predictions = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    # generate predictions asynchronously\n",
    "    predictions = df[\"sentence\"].apply(lambda x: generate_prediction(prompt, x))\n",
    "    sys_prompt_predictions.append(predictions)"
   ],
   "id": "fb821a2cfafa54c6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "Evaluating each prompt, the clear winner is the last one (`Does the given sentence reference a different scientific paper?`). Unfortunately, going strictly by metrics, it's worse than the finetuned scibert so it's hard to justify, especially considering the slower inference."
   ],
   "id": "85d462e7deb7dc56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:43:31.292639Z",
     "start_time": "2025-03-30T15:43:30.013433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for prompt, predictions in zip(prompts, sys_prompt_predictions):\n",
    "    print(prompt)\n",
    "\n",
    "    report = classification_report(df[\"citing\"], predictions)\n",
    "\n",
    "    print(report)\n",
    "    print()"
   ],
   "id": "13a8330b40fae620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the given sentence from a scientific paper missing a citation marker?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.25      0.39       470\n",
      "        True       0.07      0.93      0.14        30\n",
      "\n",
      "    accuracy                           0.29       500\n",
      "   macro avg       0.53      0.59      0.27       500\n",
      "weighted avg       0.93      0.29      0.38       500\n",
      "\n",
      "\n",
      "Does the following sentence require a citation marker?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.56      0.71       470\n",
      "        True       0.08      0.63      0.15        30\n",
      "\n",
      "    accuracy                           0.57       500\n",
      "   macro avg       0.52      0.60      0.43       500\n",
      "weighted avg       0.91      0.57      0.68       500\n",
      "\n",
      "\n",
      "Should I add a citation marker to this sentence?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.39      0.56       470\n",
      "        True       0.08      0.87      0.15        30\n",
      "\n",
      "    accuracy                           0.42       500\n",
      "   macro avg       0.53      0.63      0.36       500\n",
      "weighted avg       0.93      0.42      0.54       500\n",
      "\n",
      "\n",
      "Does the given sentence reference a different scientific paper?\n",
      "- yes\n",
      "- no\n",
      "\n",
      "Please only print the answer without anything else.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.94      0.96       470\n",
      "        True       0.38      0.57      0.45        30\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.67      0.75      0.70       500\n",
      "weighted avg       0.94      0.92      0.93       500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
