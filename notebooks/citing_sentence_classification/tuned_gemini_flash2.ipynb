{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38cde65-254d-4219-a441-068766c0d4b5",
   "metadata": {
    "id": "a38cde65-254d-4219-a441-068766c0d4b5"
   },
   "source": [
    "# Tuned Gemini Flash 2.0 for citing sentence classification\n",
    "\n",
    "This notebook attempts to determine if a Gemini instance tuned on labeled examples is capable of producing a good results in detecting citing sentences.\n",
    "\n",
    "The Gemini instance has been tuned on 10000 samples from a dataset of sentences extracted from scientific papers and labeled as citing or non-citing."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instantiation\n",
    "\n",
    "Using Google AI products requires the Google Cloud SDK to be installed on your system.\n",
    "\n",
    "The following code initializes the Vertex project (I chose the project where I have the tuned model stored) and chooses a datacenter."
   ],
   "id": "2e231ee32338a0cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:20:29.041853Z",
     "start_time": "2025-03-27T14:20:29.034259Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 57,
   "source": [
    "import vertexai\n",
    "\n",
    "# Set up the VertexAI client\n",
    "vertexai.init(\n",
    "    project=\"citingsentececlassifier\",\n",
    "    location=\"europe-west8\",\n",
    ")"
   ],
   "id": "7cd0f3913a2ace13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:00:06.837148Z",
     "start_time": "2025-03-27T15:00:06.825815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import base64\n",
    "\n",
    "def generate(content: str):\n",
    "  client = genai.Client(\n",
    "      vertexai=True,\n",
    "      project=\"438747908796\",\n",
    "      location=\"europe-west8\",\n",
    "  )\n",
    "\n",
    "\n",
    "  model = \"projects/438747908796/locations/europe-west8/endpoints/5478936809951985664\"\n",
    "  contents = [\n",
    "      content\n",
    "  ]\n",
    "  generate_content_config = types.GenerateContentConfig(\n",
    "    temperature = 0,\n",
    "    top_p = 0.95,\n",
    "    max_output_tokens = 256,\n",
    "    response_modalities = [\"TEXT\"],\n",
    "    safety_settings = [types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "      threshold=\"OFF\"\n",
    "    ),types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "      threshold=\"OFF\"\n",
    "    ),types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "      threshold=\"OFF\"\n",
    "    ),types.SafetySetting(\n",
    "      category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "      threshold=\"OFF\"\n",
    "    )],\n",
    "  )\n",
    "\n",
    "  for chunk in client.models.generate_content_stream(\n",
    "    model = model,\n",
    "    contents = contents,\n",
    "    config = generate_content_config,\n",
    "    ):\n",
    "    return chunk.text"
   ],
   "id": "a1cf3e8743d2653a",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "outputId": "a246d56c-3915-4398-8dd4-407fcf5b7c6a",
    "ExecuteTime": {
     "end_time": "2025-03-27T14:20:39.599935Z",
     "start_time": "2025-03-27T14:20:39.578977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "llm = GenerativeModel(\"citing_sentence\",\n",
    "                      generation_config={\n",
    "                      \"temperature\": 0,\n",
    "                      \"max_output_tokens\": 256,\n",
    "                      })"
   ],
   "id": "cb09c344-1836-4e0c-acf8-11d13ac1dbae",
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "id": "2b4f3e15",
   "metadata": {
    "id": "2b4f3e15"
   },
   "source": [
    "## Invocation\n",
    "\n",
    "We'll be using the model to classify a small set of sentences from scientific papers."
   ]
  },
  {
   "metadata": {
    "tags": [],
    "id": "62e0dbc3",
    "outputId": "e76997e2-c318-413b-ae3c-4a4f97ba33d8",
    "ExecuteTime": {
     "end_time": "2025-03-27T15:01:50.409312Z",
     "start_time": "2025-03-27T15:01:50.402536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_prediction(sentence: str) -> bool:\n",
    "    prompt = \"Does the given sentence reference a different scientific paper?\\n- yes\\n- no\\n\\nPlease only print the answer without anything else.\"\n",
    "    messages = prompt + sentence\n",
    "\n",
    "    return generate(messages) == \"yes\""
   ],
   "id": "62e0dbc3",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
    "outputId": "d865141c-9ef3-4a62-8183-febacea4793e",
    "ExecuteTime": {
     "end_time": "2025-03-27T15:00:14.517186Z",
     "start_time": "2025-03-27T15:00:10.245463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATASET_PATH = \"C:\\\\Users\\\\Adrian\\\\Documents\\\\datasets\\\\citing_test.parquet\"\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "df = pd.read_parquet(DATASET_PATH)\n",
    "\n",
    "# get first 500 rows\n",
    "df = df.head(500)\n",
    "\n",
    "df.describe()"
   ],
   "id": "d86145b3-bfef-46e8-b227-4dda5c9c2705",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 sentence citing\n",
       "count                                                 500    500\n",
       "unique                                                500      2\n",
       "top     Under these assumptions, we have the following...  False\n",
       "freq                                                    1    470"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>citing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Under these assumptions, we have the following...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:19:11.951648Z",
     "start_time": "2025-03-27T15:01:51.977361Z"
    }
   },
   "cell_type": "code",
   "source": "predictions = df[\"sentence\"].apply(lambda x: generate_prediction(x))",
   "id": "fb821a2cfafa54c6",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "\n",
    "Unfortunately, the model seems to have learned that saying \"no\" leads to fairly high accuracy. While this experiment is a failure, it leads us to the following question: what if we used a more balanced (but less realistic) set of examples to fine tune?"
   ],
   "id": "24902eab8837522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T15:19:31.874178Z",
     "start_time": "2025-03-27T15:19:31.858905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(df[\"citing\"], predictions)\n",
    "\n",
    "print(report)"
   ],
   "id": "13a8330b40fae620",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      1.00      0.97       470\n",
      "        True       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.94       500\n",
      "   macro avg       0.47      0.50      0.48       500\n",
      "weighted avg       0.88      0.94      0.91       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adrian\\miniconda3\\envs\\licenta\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Adrian\\miniconda3\\envs\\licenta\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Adrian\\miniconda3\\envs\\licenta\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
